{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "sns.set_theme()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "TRAIN_FILE = \"../input/nlp-getting-started/train.csv\"\n",
    "TEST_FILE = \"../input/nlp-getting-started/test.csv\"\n",
    "SUBMISSION_FILE = \"../input/nlp-getting-started/sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "train_df.rename(columns={\"target\": \"labels\"}, inplace=True)\n",
    "num_labels = len(train_df.labels.value_counts())\n",
    "train_df.sample(5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of trainining samples: {train_df.shape[0]}\")\n",
    "print(f\"Number of test samples: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (train_df, test_df):\n",
    "    df.drop([\"id\", \"keyword\", \"location\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning, module='bs4')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text() # Remove html tags\n",
    "    text = re.sub(\"http[s]?\\:\\/\\/\\S+\", \" \", text) # Remove links\n",
    "    text = re.sub(\"[ \\t\\n]+\", \" \", text) # Remove tabs, newlines and multiple spaces\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df.text = df.text.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Text length\")\n",
    "plt.xlabel(\"n words\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.hist(train_df.text.apply(lambda x: x.split(\" \")).str.len())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.train_test_split(test_size=0.1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "train_encoded = train_ds.map(tokenize, batched=True, batch_size=None)\n",
    "test_encoded = test_ds.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(batch):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(**inputs).last_hidden_state\n",
    "        \n",
    "    return {\"embedding\": pred[:, 0].cpu().numpy()}\n",
    "\n",
    "  \n",
    "train_embedding = train_encoded.map(extract_embedding, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "train_scaled = MinMaxScaler().fit_transform(train_embedding['train']['embedding'].numpy())\n",
    "reductor = umap.UMAP(n_neighbors=5, n_components=2, min_dist=0.3).fit(train_scaled)\n",
    "train_reduced = pd.DataFrame({\"X\": reductor.embedding_[:,0], \n",
    "                              \"Y\": reductor.embedding_[:,1], \n",
    "                              \"labels\": train_embedding['train']['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=train_reduced, x=\"X\", y=\"Y\", hue=\"labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_scaled, train_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_embedding['train']['embedding']\n",
    "y_train = train_embedding['train']['labels']\n",
    "\n",
    "X_valid = train_embedding['test']['embedding']\n",
    "y_valid = train_embedding['test']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter=2000, random_state=seed)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Logistic regression score: {logistic_regression.score(X_valid, y_valid):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "logging_steps = len(train_encoded[\"train\"]) // batch_size\n",
    "training_args = TrainingArguments(output_dir=\"distilbert_disaster\", \n",
    "                                  report_to=\"tensorboard\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,                                \n",
    "                                  weight_decay=0.01,\n",
    "                                  save_strategy=\"no\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level=\"error\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)  \n",
    "    acc = metric.compute(predictions=preds, references=labels)\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1_score(labels, preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_encoded[\"train\"],\n",
    "                  eval_dataset=train_encoded[\"test\"],\n",
    "                  tokenizer=tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "valid_preds = trainer.predict(train_encoded[\"test\"])\n",
    "valid_preds = np.argmax(valid_preds.predictions, axis=-1)\n",
    "\n",
    "cnf_matrix = confusion_matrix(train_encoded[\"test\"][\"labels\"], valid_preds)\n",
    "\n",
    "sns.heatmap(cnf_matrix, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"True class\")\n",
    "plt.ylabel(\"Pred class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclasified = np.nonzero(valid_preds != train_encoded[\"test\"][\"labels\"].numpy())[0][:10]\n",
    "true_labels = train_encoded[\"test\"].select(misclasified)\n",
    "pd.DataFrame({\"text\": true_labels['text'], \n",
    "              \"labels\": true_labels['labels'].numpy(), \n",
    "              \"preds\":valid_preds[misclasified]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_encoded)\n",
    "preds = np.argmax(preds.predictions, axis=-1)\n",
    "submission = pd.read_csv(SUBMISSION_FILE)\n",
    "submission.target = preds\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7591e89100990765c1e83e0bc3f2479158201ef6374fa652abf5d2d3b72a9a0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
